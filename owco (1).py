# -*- coding: utf-8 -*-
"""OWCO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tq-RmKxtxNhjS04yCNnt1UAMXle1QQCv

# Import Libraries
"""

import numpy as np
import tensorflow.keras as keras
import pandas as pd
from sklearn.model_selection import train_test_split
import os
import numpy as np
import random
from keras.layers import Dense
from keras.models import Sequential
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import pickle

"""# Define functions"""



def update_OCSI_Values(OCSIValues,loss,prevLoss,params=None):
    # print(len(OCSIValues),len(OCSIValues[0]),len(OCSIValues[1]),len(OCSIValues[2]),len(OCSIValues[3]))
    # DynamicFactors = [windSpeed * 0.000001,Temp * 0.0000001,WaterDepth * 0.00000001,waterDensity* 0.000000001]
    # MutatingFactors = [tidalForce,glacierAvalanche,earthquake]
    # print(OCSIValues[0])
    # print(OCSIValues[1])
    # print(OCSIValues[2])
    # print(OCSIValues[3])
    if not params:
        #Define initial values
        windSpeed = 0.0000729
        Temp = random.randrange(100)
        WaterDepth = random.randrange(400)
        waterDensity = 50 + random.randrange(450)
        tidalForce = random.uniform(52,110) * 10**-9
        glacierAvalanche = 10**-6 * random.uniform(1,1000)
        earthquake = 10**-7 * random.uniform(1,100)
        #create dynamic factors
        vals = [windSpeed,Temp,WaterDepth,waterDensity]
        DynamicFactors = []
        for i in range(4):
            DynamicFactors.append(10**(-6-i) * vals[i])
        #create mutating factors
        vals = [tidalForce,glacierAvalanche,earthquake]
        MutatingFactors = []
        for i in range(3):
            MutatingFactors.append(vals[i])
        params = not params
    if loss < prevLoss:
        for layer in range(len(OCSIValues)):
            for weight in range(len(OCSIValues[layer])):
                OCSIValues[layer][weight] += sum(MutatingFactors)
                OCSIValues[layer][weight] -= sum(DynamicFactors)
    else:
        for layer in range(len(OCSIValues)):
            for weight in range(len(OCSIValues[layer])):
                OCSIValues[layer][weight] -= sum(MutatingFactors)
                OCSIValues[layer][weight] += sum(DynamicFactors)
    return OCSIValues

def train(X_train, Y_train, X_test, y_test, iters, learning_rate, weights = []):
    logger = keras.callbacks.CSVLogger('error.csv', append = True)
    model = Sequential()
    model.add(Dense(12, activation='relu', input_dim = X_train.shape[1]))
    model.add(Dense(128, activation='relu'))
    model.add(Dense(Y_train.shape[1], activation='softmax'))
    model.summary()
    # model.show()
    if weights:
        model.set_weights(weights)
    model.compile(loss='categorical_crossentropy', metrics=["accuracy"])
    hist = model.fit(X_train, Y_train, epochs = iters, callbacks = [logger], shuffle =
              False, verbose = 1, validation_data=(X_test, y_test))
    return model,hist

def OWCO(X_train, Y_train, X_test, y_test, iters, iter_product):
    # Initializing PBO parameters
    #dim1 = total number of crops, dim2 = input factors
    dim1, dim2 = Y_train.shape[1], X_train.shape[1]
    learning_rate = 0.10
    #interval with which lower and upper bound will be incremented
    number_of_batches = 1
    #number of new factors to be initialised every interation
    num_factors = dim2 // number_of_batches
    #defining fan_in and fan_out for weight initialization
    fan_in, fan_out = 0, Y_train.shape[1]
    #weights array
    OCSI_Values = []
    prevLoss = 0
    for i in range(number_of_batches):
        print('Batch =====>', i + 1)
        #generate new fan_in(number of inputs)
        fan_in = (i + 1) * num_factors
        #run Adams
        model,hist = train(X_train, Y_train,X_test,y_test, iters, learning_rate, OCSI_Values)
        # osci_values = model.get_weights()
        loss = hist.history['loss'][-1]
        OCSI_Values = model.get_weights()
        updated_OCSI_Values = update_OCSI_Values(OCSI_Values,prevLoss,loss)
        model.set_weights(updated_OCSI_Values)
        #modify initial weights for the new iteration
        # new_values = update_ocsi_values(fan_in, fan_out, (num_factors, 7))[0]
        learning_rate -= 0.01
        #modify iterations
        iters *= iter_product
        prevLoss = loss
    #test this final model with our testing input
    return model.predict(X_test),hist,model

# logger = keras.callbacks.CSVLogger('error.csv', append = True)
# model = Sequential()
# model.add(Dense(10, activation='relu', input_dim = X_train.shape[1]))
# model.add(Dense(units = Y_train.shape[1], activation='softmax'))
# adop = keras.optimizers.Adam(learning_rate = learning_rate)

# model.compile(optimizer=adop, loss='categorical_crossentropy')

# a = np.array(model.get_weights())         # save weights in a np.array of np.arrays
# model.set_weights(a + 1)                  # add 1 to all weights in the neural network
# b = np.array(model.get_weights())         # save weights a second time in a np.array of np.arrays
# print(b - a)                              # print changes in weights

"""# Data Import and Preprocessing"""

pathname='crop1.csv'
data_total=pd.read_csv(pathname,header=0)

data_total = data_total.dropna()
data_total = data_total.reset_index(drop=True)

X = data_total.drop(['crop','f'], axis = 'columns')
y = data_total.crop

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2)

features = ['a','b','c','d','e','f','g','h','i','j','k','l']
Crop = ['crop']
crop_labels = ['WHEAT', 'SOYABEAN', 'RICE', 'MUSTARD', 'JOWAR', 'MAIZE','GRAM','COTTON','BARLEY','BAJRA']

# def set_up_data_set(data):

#     X_input=(np.array(data[features]))
#     Y_input=np.array(data[Crop])

#     X_input = np.array(X_input, dtype = np.float32)
#     X_mean = np.mean(X_input, axis = 0, keepdims = True)
#     X_std = np.std(X_input, axis = 0, keepdims = True)
#     X_input = (X_input - X_mean)/X_std

#     y_temp = []
#     for y in Y_input:
#         temp = [1 if y[0] == label else 0 for label in crop_labels]
#         y_temp.append(temp)
#     Y_input=np.array(y_temp)
#     print(X_input.shape)
#     print(Y_input.shape)

#     return X_input,Y_input

# X_train,Y_train=set_up_data_set(data_train)
# X_test,Y_test=set_up_data_set(data_test)

X_train.shape

y_train
y_temp = []
for y in y_train:
    temp = [1 if y == label else 0 for label in crop_labels]
    y_temp.append(temp)
Y_train=np.array(y_temp)

X_train.shape

y_temp = []
for y in y_test:
    temp = [1 if y == label else 0 for label in crop_labels]
    y_temp.append(temp)
y_test=np.array(y_temp)

"""# Model Training"""

# Select number of repetitions for each experiment.
NumOfRuns = 1
# Select parameters for Adams Optimizer
iters = 60
#every adams call, iters = iters * iter_product
iter_product = 1
#running the optimizer NumOfRuns times
for k in range (NumOfRuns):
    #returns model x that can be used to test the output
    x,history,model = OWCO(X_train, Y_train, X_test, y_test, iters, iter_product)
pickle.dump(model, open('model.pkl','wb'))

"""# Plot Graphs"""

import numpy as np
from sklearn import metrics
from sklearn import svm, datasets
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import label_binarize
from sklearn.multiclass import OneVsRestClassifier
from scipy import interp
import matplotlib.pyplot as plt

fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(10):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], x[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), x.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

plt.figure()
lw = 2
plt.plot(fpr[2], tpr[2], color='darkorange',
         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic OWCO')
plt.legend(loc="lower right")
plt.show()

import matplotlib.pyplot as plt

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Accuracy Curve')
plt.ylabel('Accuracy')
plt.xlabel('Epochs')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Curve')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()



"""# Load model for prediction"""

model = pickle.load(open('model.pkl','rb'))

#Loading model to compare the results
prediction = model.predict([[0.215179156,117.31,25.81,45.7849115,11.69764773,0.355310863,700,40,1465,0.016084871,0.479846935]])
output = np.round(prediction[0],4)

for value in prediction[0]:
    print(value)

output

y_pred = model.predict(X_test)

from sklearn.metrics import confusion_matrix
import seaborn as sns
#cm = confusion_matrix(test_y, pred_y2)
#X_train, Y_train, X_test, y_test, iters, iter_product
classes = ['WHEAT', 'SOYABEAN', 'RICE', 'MUSTARD', 'JOWAR', 'MAIZE','GRAM','COTTON','BARLEY','BAJRA']
cm = confusion_matrix(np.asarray(y_test).argmax(axis=1), np.asarray(y_pred).argmax(axis=1))
sns.heatmap(cm, annot = True, fmt='d', xticklabels=classes, yticklabels=classes)
plt.ylabel('Actual Values',color='Black')
plt.xlabel('Predicted Values')
a = cm.ravel()
#print('Accuracy =', metrics.accuracy_score(y_test,y_pred))
plt.figure(figsize=(5, 5))
plt.show()

